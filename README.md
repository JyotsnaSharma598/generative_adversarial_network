# generative_adversarial_network
## TEXT TO IMAGE GENERATION USING STACKED GENERATIVE ADVERSARIAL NETWORKS

## Objectives
- To generate image from text using Generative Adversarial Networks.
- To study network architecture of StackGAN for improved accuracy.

## Architecture of StackGAN

- It has 3 major components: Conditioning Augmentation, Stage-I GAN and Stage-II GAN. 
- First text description is converted into text embedding using a text encoder. The text encoder network encodes a sentence to a 1,024-dimensional text embedding. It is common to both the stages.
- After we get the text embeddings from the text encoder, these are fed to a fully connected layer to generate mean & standard. These are then used to create a diagonal covariance matrix. To sample augmentation variable , we first take the elementwise multiplication of std. deviation and then add the output to mean.
- The Stage-I generator network is a deep convolutional neural network with several upsampling layers. The generator network is a CGAN, which takes the Gaussian conditioning variable and random noise variable z and generates an
image of dimensions 64x64x3. The generated low-resolution image might have primitive shapes and basic colors but it will also have various defects.
- Similar to the generator network, the discriminator network is a deep convolutional neural network, containing a series of downsampling convolutional layers. The downsampling layers generate feature maps from images, whether
they are real images from the real data distribution or images generated by the generator network. Then, we concatenate the feature maps to the text embedding.
- Finally, we have a fully connected layer with one node, which is used for binary classification. The Stage-I and Stage-II Conditioning Augmentations have different fully connected layers for generating different means and standard deviations. This means that the Stage-II GAN learns to capture useful information in the text embedding that is omitted by the Stage-I GAN.
- In Stage-II Generator, The Stage-I result, which is the low-resolution image, is passed through several downsampling layers to generate image features. Then, the image features and the text conditioning variables are concatenated along the channel dimensions. After that, the concatenated tensor is fed into some residual blocks that learn multimodal representations across image and text features. Finally, the output of the last operation is fed into a set of upsampling layers, which generate a high-resolution image with dimensions of 256x256x3.

## Results

> Input Text: Bird has brown body feathers, brown breast feathers and brown beak

- Output: Even when Stage-I GAN fails to draw a plausible shape, Stage-II GAN is able to generate reasonable objects.
- We also observe that StackGAN has the ability to transfer background from Stage-I images and fine-tune them to be more realistic with higher resolution at Stage-II. After training the model of 500 epochs, it generated decent images
and after training it for 1000 epochs, it generated realistic images.

> For all input texts: Stage-I GAN sketches the object following basic color and shape constraints from given text descriptions.
> Stage-II GAN corrects the defects in Stage-I results and adds more details, yielding higher resolution images with better image quality.
> Compared to existing text-to-image generative models, this method generates higher resolution images (e.g., 256X256) with more photo-realistic details and diversity.
> Experimented result using StackGAN shows 97.74% accuracy.
